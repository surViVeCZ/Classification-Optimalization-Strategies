{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 10:46:11.906747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "################################################################################\n",
    "#                                                                              #\n",
    "#                         Author: Bc. Petr Pouƒç                                #\n",
    "#                         Date: April 4, 2024                                  #\n",
    "#                         School: Brno University of Technology (BUT)          #\n",
    "#                                                                              #\n",
    "#         Master's Thesis: Optimization of Classification Models               #\n",
    "#                         for Malicious Domain Detection                       #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "sys.path.append('..')  # Simplify the addition of the path to sys.path\n",
    "from utils.preprocess_one_domain import NDF\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline and Feature Selection\n",
    "\n",
    "This document outlines the preprocessing and feature engineering steps implemented in the given Python script for handling datasets stored in Parquet files, focusing on cybersecurity data distinguishing between benign and malign behaviors. The script incorporates extensive data processing capabilities, including missing value imputation, outlier removal, categorical variable encoding, scaling, and advanced feature generation through decision tree predictions.\n",
    "\n",
    "## Overview of Processing Steps\n",
    "\n",
    "### Initial Setup and Imports:\n",
    "- Import necessary libraries for data handling (e.g., Pandas, PyArrow), visualization (e.g., Matplotlib, Seaborn), machine learning (e.g., scikit-learn, XGBoost), and utility functions.\n",
    "- Set up logging and warning suppression to streamline output.\n",
    "\n",
    "### Class Structure and Initialization:\n",
    "- Define a class `FeatureEngineeringCLI` for the preprocessing pipeline, capable of handling either single-record inputs or full datasets.\n",
    "- Initialize class attributes for paths, scalers, outlier detectors, and machine learning models.\n",
    "\n",
    "### Data Preprocessing Steps:\n",
    "1. **Data Loading**: Load benign and malign datasets from specified Parquet files.\n",
    "2. **Feature Cleaning and Engineering**:\n",
    "   - Remove non-training columns to focus on relevant features.\n",
    "   - Handle missing values by setting them to a default value of `-1`.\n",
    "   - Identify and remove outliers based on standard deviation thresholds.\n",
    "   - Encode categorical variables using techniques such as one-hot encoding and binary encoding.\n",
    "   - Generate new features using predictions from a trained decision tree classifier.\n",
    "3. **Scaling and Transformation**:\n",
    "   - Apply different scaling techniques (StandardScaler, MinMaxScaler, RobustScaler) depending on the dataset characteristics and the downstream machine learning model requirements.\n",
    "   - For CNN models, recommend using MinMax scaling followed by a sigmoid transformation to normalize inputs.\n",
    "4. **Model Training and Evaluation**:\n",
    "   - Train a decision tree classifier on the processed features to generate a new feature reflecting the likelihood of malign behavior.\n",
    "   - Evaluate the decision tree model's accuracy and cross-validation scores to ensure robust performance.\n",
    "\n",
    "### Saving and Loading Models:\n",
    "- Implement functions to save and load scalers, outlier detection thresholds, and the decision tree model to ensure reproducibility and efficiency in processing new data.\n",
    "\n",
    "### Utility Functions:\n",
    "- Provide additional helper methods for categorical encoding, timestamp handling, and logging.\n",
    "\n",
    "### Execution and Output:\n",
    "- Detail the process for running the pipeline, including handling command-line arguments and producing a final processed dataset ready for machine learning model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 10:46:16,892 - utils.preprocess_one_domain - INFO - Benign dataset path: ../floor/benign_2312.parquet\n",
      "2024-05-11 10:46:16,895 - utils.preprocess_one_domain - INFO - Malign dataset path: ../floor/phishing_2311.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malign dataset path: ../floor/phishing_2311.parquet\n",
      "Benign dataset path: ../floor/benign_2312.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 10:46:17,967 - utils.preprocess_one_domain - INFO - Number of records in benign dataset: 432572\n",
      "2024-05-11 10:46:17,970 - utils.preprocess_one_domain - INFO - Number of records in malign dataset: 68353\n",
      "2024-05-11 10:46:21,220 - utils.preprocess_one_domain - INFO - Total percentage of missing values in benign dataset: 0.39%\n",
      "2024-05-11 10:46:21,224 - utils.preprocess_one_domain - INFO - Total percentage of missing values in malign dataset: 0.45%\n",
      "2024-05-11 10:46:36,967 - utils.preprocess_one_domain - INFO - Decision tree model saved to trained_borders/decision_tree_model.joblib\n",
      "2024-05-11 10:46:37,337 - utils.preprocess_one_domain - INFO - New feature 'dtree_prob' created from decision tree predictions.\n",
      "2024-05-11 10:46:38,250 - utils.preprocess_one_domain - INFO - Decision Tree Train Accuracy: 0.94\n",
      "2024-05-11 10:46:38,252 - utils.preprocess_one_domain - INFO - Decision Tree Test Accuracy: 0.93\n",
      "2024-05-11 10:46:59,866 - utils.preprocess_one_domain - INFO - Decision Tree Cross-Validation Scores: [0.92957029 0.92992364 0.92961821]\n",
      "2024-05-11 10:46:59,882 - utils.preprocess_one_domain - INFO - Generated class map: {'benign_2310:unknown': 0, 'misp_2310:phishing': 1}\n",
      "2024-05-11 10:47:07,049 - utils.preprocess_one_domain - INFO - Outliers thresholds saved to trained_borders/outliers.joblib\n",
      "2024-05-11 10:47:07,467 - utils.preprocess_one_domain - INFO - Outliers removed from dns_A_count: 520 rows\n",
      "2024-05-11 10:47:07,679 - utils.preprocess_one_domain - INFO - Outliers removed from dns_AAAA_count: 33 rows\n",
      "2024-05-11 10:47:07,885 - utils.preprocess_one_domain - INFO - Outliers removed from dns_MX_count: 113 rows\n",
      "2024-05-11 10:47:08,090 - utils.preprocess_one_domain - INFO - Outliers removed from dns_NS_count: 27 rows\n",
      "2024-05-11 10:47:08,289 - utils.preprocess_one_domain - INFO - Outliers removed from dns_TXT_count: 953 rows\n",
      "2024-05-11 10:47:08,955 - utils.preprocess_one_domain - INFO - Outliers removed from dns_zone_level: 138 rows\n",
      "2024-05-11 10:47:09,168 - utils.preprocess_one_domain - INFO - Outliers removed from dns_zone_digit_count: 707 rows\n",
      "2024-05-11 10:47:09,375 - utils.preprocess_one_domain - INFO - Outliers removed from dns_zone_len: 3 rows\n",
      "2024-05-11 10:47:10,134 - utils.preprocess_one_domain - INFO - Outliers removed from dns_ttl_avg: 11 rows\n",
      "2024-05-11 10:47:11,193 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_primary_ns_level: 1 rows\n",
      "2024-05-11 10:47:11,418 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_primary_ns_digit_count: 8 rows\n",
      "2024-05-11 10:47:12,047 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_email_level: 4 rows\n",
      "2024-05-11 10:47:12,261 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_email_digit_count: 783 rows\n",
      "2024-05-11 10:47:12,873 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_refresh: 2 rows\n",
      "2024-05-11 10:47:13,119 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_retry: 94 rows\n",
      "2024-05-11 10:47:13,388 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_expire: 45 rows\n",
      "2024-05-11 10:47:13,696 - utils.preprocess_one_domain - INFO - Outliers removed from dns_soa_min_ttl: 2 rows\n",
      "2024-05-11 10:47:13,991 - utils.preprocess_one_domain - INFO - Outliers removed from dns_domain_name_in_mx: 3197 rows\n",
      "2024-05-11 10:47:15,027 - utils.preprocess_one_domain - INFO - Outliers removed from dns_txt_external_verification_score: 405 rows\n",
      "2024-05-11 10:47:15,491 - utils.preprocess_one_domain - INFO - Outliers removed from dns_txt_dkim_exists: 5933 rows\n",
      "2024-05-11 10:47:15,701 - utils.preprocess_one_domain - INFO - Outliers removed from dns_txt_dmarc_exists: 785 rows\n",
      "2024-05-11 10:47:15,892 - utils.preprocess_one_domain - INFO - Outliers removed from ip_count: 124 rows\n",
      "2024-05-11 10:47:16,099 - utils.preprocess_one_domain - INFO - Outliers removed from ip_mean_average_rtt: 9 rows\n",
      "2024-05-11 10:47:17,241 - utils.preprocess_one_domain - INFO - Outliers removed from ip_distinct_as_count: 51 rows\n",
      "2024-05-11 10:47:19,063 - utils.preprocess_one_domain - INFO - Outliers removed from tls_iso_policy_crt_count: 1 rows\n",
      "2024-05-11 10:47:19,867 - utils.preprocess_one_domain - INFO - Outliers removed from tls_leaf_cert_validity_len: 127 rows\n",
      "2024-05-11 10:47:21,074 - utils.preprocess_one_domain - INFO - Outliers removed from tls_root_cert_validity_len: 5 rows\n",
      "2024-05-11 10:47:21,544 - utils.preprocess_one_domain - INFO - Outliers removed from tls_subject_count: 1241 rows\n",
      "2024-05-11 10:47:21,920 - utils.preprocess_one_domain - INFO - Outliers removed from tls_unique_SLD_count: 660 rows\n",
      "2024-05-11 10:47:22,498 - utils.preprocess_one_domain - INFO - Outliers removed from lex_name_len: 4 rows\n",
      "2024-05-11 10:47:22,887 - utils.preprocess_one_domain - INFO - Outliers removed from lex_phishing_keyword_count: 25 rows\n",
      "2024-05-11 10:47:23,088 - utils.preprocess_one_domain - INFO - Outliers removed from lex_benign_keyword_count: 19 rows\n",
      "2024-05-11 10:47:23,276 - utils.preprocess_one_domain - INFO - Outliers removed from lex_consecutive_chars: 59 rows\n",
      "2024-05-11 10:47:23,515 - utils.preprocess_one_domain - INFO - Outliers removed from lex_tld_len: 453 rows\n",
      "2024-05-11 10:47:24,062 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sld_len: 27 rows\n",
      "2024-05-11 10:47:24,439 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sld_digit_count: 921 rows\n",
      "2024-05-11 10:47:24,645 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sld_digit_ratio: 1414 rows\n",
      "2024-05-11 10:47:24,856 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sld_phishing_keyword_count: 108 rows\n",
      "2024-05-11 10:47:25,431 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sld_consonant_count: 249 rows\n",
      "2024-05-11 10:47:25,833 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sld_non_alphanum_count: 263 rows\n",
      "2024-05-11 10:47:26,030 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sld_non_alphanum_ratio: 275 rows\n",
      "2024-05-11 10:47:26,613 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sub_count: 3 rows\n",
      "2024-05-11 10:47:27,426 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sub_max_consonant_len: 108 rows\n",
      "2024-05-11 10:47:27,877 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sub_digit_count: 519 rows\n",
      "2024-05-11 10:47:28,272 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sub_vowel_count: 4 rows\n",
      "2024-05-11 10:47:28,672 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sub_consonant_count: 1 rows\n",
      "2024-05-11 10:47:29,085 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sub_non_alphanum_count: 25 rows\n",
      "2024-05-11 10:47:29,505 - utils.preprocess_one_domain - INFO - Outliers removed from lex_sub_hex_count: 4 rows\n",
      "2024-05-11 10:47:30,431 - utils.preprocess_one_domain - INFO - Outliers removed from lex_phishing_tetragram_matches: 11 rows\n",
      "2024-05-11 10:47:30,637 - utils.preprocess_one_domain - INFO - Outliers removed from lex_phishing_pentagram_matches: 4 rows\n",
      "2024-05-11 10:47:31,243 - utils.preprocess_one_domain - INFO - Outliers removed from lex_malware_tetragram_matches: 21 rows\n",
      "2024-05-11 10:47:31,855 - utils.preprocess_one_domain - INFO - Outliers removed from lex_dga_tetragram_matches: 87 rows\n",
      "2024-05-11 10:47:32,051 - utils.preprocess_one_domain - INFO - Outliers removed from lex_avg_part_len: 3 rows\n",
      "2024-05-11 10:47:32,674 - utils.preprocess_one_domain - INFO - Outliers removed from lex_short_part_count: 6 rows\n",
      "2024-05-11 10:47:33,274 - utils.preprocess_one_domain - INFO - Outliers removed from lex_superlong_part_count: 1 rows\n",
      "2024-05-11 10:47:33,665 - utils.preprocess_one_domain - INFO - Outliers removed from lex_ipv4_in_domain: 828 rows\n",
      "2024-05-11 10:47:34,461 - utils.preprocess_one_domain - INFO - Outliers removed from lex_has_vps_suffix: 2678 rows\n",
      "2024-05-11 10:47:34,743 - utils.preprocess_one_domain - INFO - Outliers removed from lex_has_img_suffix: 853 rows\n",
      "2024-05-11 10:47:35,125 - utils.preprocess_one_domain - INFO - Outliers removed from geo_countries_count: 12 rows\n",
      "2024-05-11 10:47:35,703 - utils.preprocess_one_domain - INFO - Outliers removed from geo_lat_stdev: 788 rows\n",
      "2024-05-11 10:47:38,271 - utils.preprocess_one_domain - INFO - Outliers removed from geo_continent_hash: 1 rows\n",
      "2024-05-11 10:47:38,669 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_registration_period: 7 rows\n",
      "2024-05-11 10:47:39,188 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_time_from_last_change: 305 rows\n",
      "2024-05-11 10:47:39,592 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_has_dnssec: 5200 rows\n",
      "2024-05-11 10:47:40,356 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_registrant_name_len: 30 rows\n",
      "2024-05-11 10:47:40,766 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_admin_name_len: 2283 rows\n",
      "2024-05-11 10:47:40,979 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_admin_name_entropy: 1237 rows\n",
      "2024-05-11 10:47:41,214 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_admin_email_len: 95 rows\n",
      "2024-05-11 10:47:41,592 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_ip_v4_count: 11 rows\n",
      "2024-05-11 10:47:41,786 - utils.preprocess_one_domain - INFO - Outliers removed from rdap_ip_v6_count: 10 rows\n",
      "2024-05-11 10:47:43,499 - utils.preprocess_one_domain - INFO - Completed outlier removal.\n",
      "2024-05-11 10:47:48,714 - utils.preprocess_one_domain - INFO - Applying MinMaxScaler + Sigmoid scaling to the features.\n",
      "2024-05-11 10:47:51,128 - utils.preprocess_one_domain - INFO - Scaler saved to trained_borders/scaler.joblib\n",
      "2024-05-11 10:47:51,136 - utils.preprocess_one_domain - INFO - Scaling applied to the features\n",
      "\n",
      "2024-05-11 10:47:53,329 - utils.preprocess_one_domain - INFO - Modified combined dataset saved to modified_dataset.parquet\n",
      "2024-05-11 10:47:53,331 - utils.preprocess_one_domain - INFO - Head of modified combined dataset:\n",
      "2024-05-11 10:47:53,334 - utils.preprocess_one_domain - INFO -         dns_has_dnskey  dns_A_count  dns_AAAA_count  dns_MX_count  \\\n",
      "0             0.500000     0.517850             0.5      0.522712   \n",
      "1             0.500000     0.517850             0.5      0.500000   \n",
      "2             0.500000     0.500000             0.5      0.500000   \n",
      "3             0.500000     0.500000             0.5      0.522712   \n",
      "4             0.500000     0.500000             0.5      0.500000   \n",
      "...                ...          ...             ...           ...   \n",
      "500919        0.500000     0.535654             0.5      0.500000   \n",
      "500920        0.500000     0.500000             0.5      0.500000   \n",
      "500922        0.500000     0.500000             0.5      0.500000   \n",
      "500923        0.500000     0.500000             0.5      0.500000   \n",
      "500924        0.731059     0.517850             0.5      0.500000   \n",
      "\n",
      "        dns_NS_count  dns_TXT_count  dns_SOA_count  dns_CNAME_count  \\\n",
      "0           0.538386       0.510415       0.731059         0.500000   \n",
      "1           0.500000       0.500000       0.500000         0.500000   \n",
      "2           0.500000       0.500000       0.500000         0.731059   \n",
      "3           0.500000       0.510415       0.500000         0.500000   \n",
      "4           0.500000       0.500000       0.500000         0.731059   \n",
      "...              ...            ...            ...              ...   \n",
      "500919      0.613379       0.520821       0.731059         0.500000   \n",
      "500920      0.500000       0.500000       0.500000         0.500000   \n",
      "500922      0.500000       0.500000       0.500000         0.500000   \n",
      "500923      0.500000       0.500000       0.500000         0.731059   \n",
      "500924      0.500000       0.500000       0.500000         0.500000   \n",
      "\n",
      "        dns_zone_level  dns_zone_digit_count  ...  rdap_ip_v6_count  \\\n",
      "0             0.562177              0.500000  ...          0.500000   \n",
      "1             0.500000              0.500000  ...          0.500000   \n",
      "2             0.562177              0.639093  ...          0.500000   \n",
      "3             0.500000              0.500000  ...          0.500000   \n",
      "4             0.500000              0.500000  ...          0.500000   \n",
      "...                ...                   ...  ...               ...   \n",
      "500919        0.500000              0.500000  ...          0.562177   \n",
      "500920        0.500000              0.500000  ...          0.500000   \n",
      "500922        0.562177              0.500000  ...          0.500000   \n",
      "500923        0.500000              0.500000  ...          0.500000   \n",
      "500924        0.500000              0.500000  ...          0.500000   \n",
      "\n",
      "        rdap_ip_shortest_v4_prefix_len  rdap_ip_longest_v4_prefix_len  \\\n",
      "0                             0.600188                       0.600188   \n",
      "1                             0.658418                       0.658418   \n",
      "2                             0.622459                       0.622459   \n",
      "3                             0.600188                       0.600188   \n",
      "4                             0.622459                       0.622459   \n",
      "...                                ...                            ...   \n",
      "500919                        0.585101                       0.679179   \n",
      "500920                        0.500000                       0.500000   \n",
      "500922                        0.500000                       0.500000   \n",
      "500923                        0.585101                       0.585101   \n",
      "500924                        0.629775                       0.629775   \n",
      "\n",
      "        rdap_ip_shortest_v6_prefix_len  rdap_ip_longest_v6_prefix_len  \\\n",
      "0                             0.500000                         0.5000   \n",
      "1                             0.500000                         0.5000   \n",
      "2                             0.500000                         0.5000   \n",
      "3                             0.500000                         0.5000   \n",
      "4                             0.500000                         0.5000   \n",
      "...                                ...                            ...   \n",
      "500919                        0.546738                         0.5564   \n",
      "500920                        0.500000                         0.5000   \n",
      "500922                        0.500000                         0.5000   \n",
      "500923                        0.500000                         0.5000   \n",
      "500924                        0.500000                         0.5000   \n",
      "\n",
      "        rdap_ip_avg_admin_name_len  rdap_ip_avg_admin_name_entropy  \\\n",
      "0                         0.523533                        0.558676   \n",
      "1                         0.550551                        0.607662   \n",
      "2                         0.539771                        0.630112   \n",
      "3                         0.589602                        0.563329   \n",
      "4                         0.500000                        0.500000   \n",
      "...                            ...                             ...   \n",
      "500919                    0.514489                        0.596193   \n",
      "500920                    0.500000                        0.500000   \n",
      "500922                    0.500000                        0.500000   \n",
      "500923                    0.568409                        0.588146   \n",
      "500924                    0.539771                        0.630112   \n",
      "\n",
      "        rdap_ip_avg_admin_email_len  rdap_ip_avg_admin_email_entropy  \\\n",
      "0                          0.555064                         0.556160   \n",
      "1                          0.640359                         0.594258   \n",
      "2                          0.599611                         0.626038   \n",
      "3                          0.622459                         0.603090   \n",
      "4                          0.500000                         0.500000   \n",
      "...                             ...                              ...   \n",
      "500919                     0.552009                         0.574724   \n",
      "500920                     0.500000                         0.500000   \n",
      "500922                     0.500000                         0.500000   \n",
      "500923                     0.617930                         0.611100   \n",
      "500924                     0.599611                         0.626038   \n",
      "\n",
      "        dtree_prob  \n",
      "0         0.500108  \n",
      "1         0.501224  \n",
      "2         0.500000  \n",
      "3         0.507812  \n",
      "4         0.500893  \n",
      "...            ...  \n",
      "500919    0.500000  \n",
      "500920    0.506174  \n",
      "500922    0.506174  \n",
      "500923    0.500000  \n",
      "500924    0.501134  \n",
      "\n",
      "[465991 rows x 179 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Subset:\n",
      "Name: dataset_../floor/benign2312_../floor/phishing2311_2024-05-11.parquet\n",
      "Features:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "0        0.5   0.535654        0.5   0.522712   0.576322   0.551896   \n",
      "1        0.5   0.500000        0.5   0.500000   0.576322   0.510415   \n",
      "2        0.5   0.517850        0.5   0.500000   0.500000   0.500000   \n",
      "3        0.5   0.517850        0.5   0.500000   0.500000   0.500000   \n",
      "4        0.5   0.500000        0.5   0.500000   0.500000   0.500000   \n",
      "5        0.5   0.517850        0.5   0.500000   0.557438   0.582570   \n",
      "6        0.5   0.517850        0.5   0.500000   0.500000   0.500000   \n",
      "7        0.5   0.500000        0.5   0.500000   0.500000   0.500000   \n",
      "8        0.5   0.500000        0.5   0.500000   0.500000   0.500000   \n",
      "9        0.5   0.500000        0.5   0.500000   0.500000   0.500000   \n",
      "\n",
      "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_169  Feature_170  \\\n",
      "0   0.731059        0.5   0.500000   0.500000  ...      0.54157     0.577495   \n",
      "1   0.731059        0.5   0.500000   0.500000  ...      0.54157     0.658418   \n",
      "2   0.500000        0.5   0.679179   0.535654  ...      0.50000     0.629775   \n",
      "3   0.500000        0.5   0.562177   0.500000  ...      0.50000     0.622459   \n",
      "4   0.500000        0.5   0.500000   0.500000  ...      0.50000     0.500000   \n",
      "5   0.731059        0.5   0.500000   0.500000  ...      0.50000     0.644225   \n",
      "6   0.500000        0.5   0.679179   0.535654  ...      0.50000     0.622459   \n",
      "7   0.500000        0.5   0.500000   0.500000  ...      0.50000     0.500000   \n",
      "8   0.500000        0.5   0.500000   0.500000  ...      0.50000     0.500000   \n",
      "9   0.500000        0.5   0.500000   0.500000  ...      0.50000     0.500000   \n",
      "\n",
      "   Feature_171  Feature_172  Feature_173  Feature_174  Feature_175  \\\n",
      "0     0.672332      0.55447     0.590779     0.554135     0.607170   \n",
      "1     0.672332      0.55447     0.590779     0.546963     0.615758   \n",
      "2     0.629775      0.50000     0.500000     0.589602     0.569643   \n",
      "3     0.622459      0.50000     0.500000     0.536169     0.652211   \n",
      "4     0.500000      0.50000     0.500000     0.500000     0.500000   \n",
      "5     0.644225      0.50000     0.500000     0.532563     0.650203   \n",
      "6     0.622459      0.50000     0.500000     0.589602     0.569643   \n",
      "7     0.500000      0.50000     0.500000     0.500000     0.500000   \n",
      "8     0.500000      0.50000     0.500000     0.500000     0.500000   \n",
      "9     0.500000      0.50000     0.500000     0.500000     0.500000   \n",
      "\n",
      "   Feature_176  Feature_177  Feature_178  \n",
      "0     0.611096     0.609620     0.514847  \n",
      "1     0.608809     0.610921     0.506329  \n",
      "2     0.599611     0.629229     0.501134  \n",
      "3     0.590343     0.635593     0.502907  \n",
      "4     0.500000     0.500000     0.540521  \n",
      "5     0.581011     0.643580     0.500000  \n",
      "6     0.599611     0.629229     0.501134  \n",
      "7     0.500000     0.500000     0.500453  \n",
      "8     0.500000     0.500000     0.500000  \n",
      "9     0.500000     0.500000     0.540521  \n",
      "\n",
      "[10 rows x 179 columns]\n",
      "Labels:\n",
      "   Label\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "5    0.0\n",
      "6    0.0\n",
      "7    0.0\n",
      "8    0.0\n",
      "9    0.0\n",
      "Dimension: 179\n",
      "Index(['dns_has_dnskey', 'dns_A_count', 'dns_AAAA_count', 'dns_MX_count',\n",
      "       'dns_NS_count', 'dns_TXT_count', 'dns_SOA_count', 'dns_CNAME_count',\n",
      "       'dns_zone_level', 'dns_zone_digit_count',\n",
      "       ...\n",
      "       'rdap_ip_v6_count', 'rdap_ip_shortest_v4_prefix_len',\n",
      "       'rdap_ip_longest_v4_prefix_len', 'rdap_ip_shortest_v6_prefix_len',\n",
      "       'rdap_ip_longest_v6_prefix_len', 'rdap_ip_avg_admin_name_len',\n",
      "       'rdap_ip_avg_admin_name_entropy', 'rdap_ip_avg_admin_email_len',\n",
      "       'rdap_ip_avg_admin_email_entropy', 'dtree_prob'],\n",
      "      dtype='object', length=179)\n",
      "torch.Size([372792, 179])\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "(tensor([0., 1.]), tensor([327911,  44881]))\n"
     ]
    }
   ],
   "source": [
    "input_data = {\n",
    "    'benign': '../floor/benign_2312.parquet',\n",
    "    'malign': '../floor/phishing_2311.parquet'\n",
    "}\n",
    "dataset = NDF(\"cnn\", True, input_data=input_data, one_line_processing=False)\n",
    "\n",
    "print(dataset['feature_names'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(torch.Tensor(dataset['features']), torch.Tensor(dataset['labels']), test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "#print labels of z?train, values distribution\n",
    "print(y_train)\n",
    "print(y_train.unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamically calculating the dimensions required to reshape the input data into a suitable format for a CNN, based on the number of features. This reshaping is necessary because CNNs expect image data, so we treat each data point as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([465991, 179])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dataset['features'].shape)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Function to calculate the next perfect square greater than a given number\n",
    "def next_perfect_square(n):\n",
    "    next_square = math.ceil(n**0.5)**2\n",
    "    return next_square\n",
    "\n",
    "# Calculate the side size for the square shape dynamically\n",
    "feature_size = x_train.shape[1]  # Number of features in your dataset\n",
    "desired_size = next_perfect_square(feature_size)  # Next perfect square\n",
    "side_size = int(desired_size**0.5)  # Side size of the square\n",
    "\n",
    "# Calculate padding required to achieve the desired size\n",
    "padding = desired_size - feature_size\n",
    "\n",
    "# Applying dynamic padding\n",
    "if padding > 0:\n",
    "    # The padding is applied to the last dimension of the dataset\n",
    "    # (0, padding) applies the padding only to the right side of the last dimension\n",
    "    x_train_padded = F.pad(x_train, (0, padding), 'constant', 0)\n",
    "    x_test_padded = F.pad(x_test, (0, padding), 'constant', 0)\n",
    "else:\n",
    "    # If no padding is needed, use the original data\n",
    "    x_train_padded = x_train\n",
    "    x_test_padded = x_test\n",
    "\n",
    "# Reshape the data to the new dynamically calculated square shape\n",
    "x_train = x_train_padded.view(-1, 1, side_size, side_size)\n",
    "x_test = x_test_padded.view(-1, 1, side_size, side_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.08942618751618611, Accuracy: 0.9705, F1 Score: 0.9697\n",
      "Epoch 2, Loss: 0.059688577547073844, Accuracy: 0.9805, F1 Score: 0.9802\n",
      "Epoch 3, Loss: 0.05284278821058236, Accuracy: 0.9829, F1 Score: 0.9827\n",
      "Accuracy on test set: 98.47530552902928%\n",
      "Predicted classes: [0 0 0 ... 0 0 0]\n",
      "Probabilities: [[0.999 0.001]\n",
      " [1.    0.   ]\n",
      " [1.    0.   ]\n",
      " ...\n",
      " [0.998 0.002]\n",
      " [1.    0.   ]\n",
      " [0.997 0.003]]\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, side_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * (side_size-4)**2, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Net(side_size=side_size)  # Ensure side_size is defined based on your input reshaping logic\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Ensure model is in training mode\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for data, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = sum(p == t for p, t in zip(all_predictions, all_targets))\n",
    "    accuracy = correct / len(all_targets)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets.long()).sum().item()\n",
    "\n",
    "    print(f'Accuracy on test set: {100 * correct / total}%')\n",
    "\n",
    "    # Predictions and probabilities for the test set\n",
    "    outputs = model(x_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_np = predicted.numpy()\n",
    "\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    probabilities_np = probabilities.numpy()\n",
    "    probabilities_np_rounded = np.round(probabilities_np, decimals=3)\n",
    "\n",
    "    print(\"Predicted classes:\", predicted_np)\n",
    "    print(\"Probabilities:\", probabilities_np_rounded)\n",
    "\n",
    "\n",
    "\n",
    "model_path = 'models/ndf_classification.pth'\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a Single Domain\n",
    "\n",
    "This section explains how to process a single domain using previously saved boundaries and models. The process involves reading a single record from a Parquet file and applying the preprocessing steps defined in the NDF function.\n",
    "\n",
    "### Steps Included in Processing:\n",
    "\n",
    "- **Parquet File Reading**: A specific Parquet file is read into a pandas DataFrame.\n",
    "- **Record Selection**: The first record of the DataFrame is selected for processing.\n",
    "- **Preprocessing Application**: The NDF function is called with parameters set for processing a single domain, including scaling and encoding based on saved boundaries and models.\n",
    "\n",
    "#### Using `joblib` for Model Persistence:\n",
    "- **Saving Models**: After training a model (e.g., a decision tree classifier), save it using `joblib.dump(model, 'model_filename.joblib')`. This makes it easy to preserve the state of the model for future use.\n",
    "- **Loading Models**: When processing a new single domain record, load the saved model using `joblib.load('model_filename.joblib')`. This ensures that the exact configurations and learned patterns of the model are applied to the new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the single record...\n",
      "{'domain_name': 'google.com', 'label': 'benign_2310:unknown', 'dns_has_dnskey': 0.0, 'dns_A_count': 1, 'dns_AAAA_count': 1, 'dns_MX_count': 1, 'dns_NS_count': 4, 'dns_TXT_count': 12, 'dns_SOA_count': 1, 'dns_CNAME_count': 0, 'dns_zone_level': 0, 'dns_zone_digit_count': 0, 'dns_zone_len': 10, 'dns_zone_entropy': 0.26464393446710155, 'dns_resolved_record_types': 6, 'dns_dnssec_score': 0.0, 'dns_ttl_avg': 58360.0, 'dns_ttl_stdev': 128463.52011368831, 'dns_ttl_low': 0.16666666666666666, 'dns_ttl_mid': 0.5, 'dns_ttl_distinct_count': 4.0, 'dns_soa_primary_ns_level': 1.0, 'dns_soa_primary_ns_digit_count': 1.0, 'dns_soa_primary_ns_len': 14.0, 'dns_soa_primary_ns_entropy': 0.22728612962572958, 'dns_soa_email_level': 1.0, 'dns_soa_email_digit_count': 0.0, 'dns_soa_email_len': 20.0, 'dns_soa_email_entropy': 0.17920918598895944, 'dns_soa_refresh': 900.0, 'dns_soa_retry': 900.0, 'dns_soa_expire': 1800.0, 'dns_soa_min_ttl': 60.0, 'dns_domain_name_in_mx': False, 'dns_mx_avg_len': 15.0, 'dns_mx_avg_entropy': 0.21265987303095246, 'dns_txt_avg_len': 57.5, 'dns_txt_avg_entropy': 0.08240880501807765, 'dns_txt_external_verification_score': 5, 'dns_txt_spf_exists': 1, 'dns_txt_dkim_exists': 0, 'dns_txt_dmarc_exists': 0, 'ip_count': 39, 'ip_mean_average_rtt': 29.7491282051282, 'ip_v4_ratio': 0.5, 'ip_a_aaaa_to_all_ratio': 0.05128205128205128, 'ip_entropy': 4.732644606657733, 'ip_as_address_entropy': 2.7760178872758647, 'ip_asn_entropy': -0.0, 'ip_distinct_as_count': 1.0, 'tls_CA_certs_in_chain_ratio': 0.0, 'tls_broken_chain': 0.0, 'tls_chain_len': 3.0, 'tls_client_auth_crt_count': 0.0, 'tls_common_name_count': 0.0, 'tls_critical_extensions': -1.0, 'tls_expired_chain': 1.0, 'tls_has_tls': True, 'tls_is_self_signed': 0.0, 'tls_iso_policy_crt_count': 0.0, 'tls_joint_isoitu_policy_crt_count': 0.0, 'tls_leaf_authority_hash': 484934813.0, 'tls_leaf_cert_lifetime': nan, 'tls_leaf_cert_validity_len': -1.0, 'tls_negotiated_cipher_id': 2.0, 'tls_negotiated_version_id': 3.0, 'tls_percentage_crt_with_policies': 0.0, 'tls_root_authority_hash': 484934813.0, 'tls_root_cert_lifetime': nan, 'tls_root_cert_validity_len': -1.0, 'tls_server_auth_crt_count': 0.0, 'tls_subject_count': 0.0, 'tls_total_extension_count': -1.0, 'tls_unique_SLD_count': 0.0, 'tls_with_policies_crt_count': 0.0, 'tls_x509_anypolicy_crt_count': 0.0, 'lex_name_len': 10, 'lex_has_digit': 0, 'lex_phishing_keyword_count': 0, 'lex_benign_keyword_count': 0, 'lex_consecutive_chars': 2, 'lex_tld_len': 3, 'lex_tld_abuse_score': 0.6554, 'lex_tld_hash': 3649657627, 'lex_sld_len': 6, 'lex_sld_norm_entropy': 0.3197159723424149, 'lex_sld_digit_count': 0.0, 'lex_sld_digit_ratio': 0.0, 'lex_sld_phishing_keyword_count': 0, 'lex_sld_vowel_count': 3, 'lex_sld_vowel_ratio': 0.5, 'lex_sld_consonant_count': 3, 'lex_sld_consonant_ratio': 0.5, 'lex_sld_non_alphanum_count': 0, 'lex_sld_non_alphanum_ratio': 0.0, 'lex_sld_hex_count': 1, 'lex_sld_hex_ratio': 0.16666666666666666, 'lex_sub_count': 0, 'lex_stld_unique_char_count': 6, 'lex_begins_with_digit': 0, 'lex_www_flag': 0, 'lex_sub_max_consonant_len': 2, 'lex_sub_norm_entropy': 0.3197159723424149, 'lex_sub_digit_count': 0.0, 'lex_sub_digit_ratio': 0.0, 'lex_sub_vowel_count': 3, 'lex_sub_vowel_ratio': 0.5, 'lex_sub_consonant_count': 3, 'lex_sub_consonant_ratio': 0.5, 'lex_sub_non_alphanum_count': 0, 'lex_sub_non_alphanum_ratio': 0.0, 'lex_sub_hex_count': 1, 'lex_sub_hex_ratio': 0.16666666666666666, 'lex_phishing_bigram_matches': 5, 'lex_phishing_trigram_matches': 2, 'lex_phishing_tetragram_matches': 2, 'lex_phishing_pentagram_matches': 2, 'lex_malware_bigram_matches': 5, 'lex_malware_trigram_matches': 4, 'lex_malware_tetragram_matches': 2, 'lex_dga_bigram_matches': 5, 'lex_dga_trigram_matches': 2, 'lex_dga_tetragram_matches': 0, 'lex_avg_part_len': 4.5, 'lex_stdev_part_lens': 1.5, 'lex_longest_part_len': 6, 'lex_short_part_count': 1, 'lex_medium_part_count': 1, 'lex_long_part_count': 0, 'lex_superlong_part_count': 0, 'lex_shortest_sub_len': 6, 'lex_ipv4_in_domain': 0, 'lex_has_trusted_suffix': 1, 'lex_has_wellknown_suffix': 0, 'lex_has_cdn_suffix': 0, 'lex_has_vps_suffix': 0, 'lex_has_img_suffix': 0, 'lex_suffix_score': 10, 'geo_countries_count': 3, 'geo_continents_count': 2, 'geo_malic_host_country': 1, 'geo_lat_stdev': 7.307564580722723, 'geo_lon_stdev': 47.23635212626406, 'geo_mean_lat': 41.086889743589744, 'geo_mean_lon': -74.29190000000001, 'geo_min_lat': 34.0544, 'geo_max_lat': 34.0544, 'geo_min_lon': -118.244, 'geo_max_lon': -118.244, 'geo_lat_range': 0.0, 'geo_lon_range': 0.0, 'geo_centroid_lat': 51.0816, 'geo_centroid_lon': -177.36599999999999, 'geo_estimated_area': 0.0, 'geo_continent_hash': 4, 'geo_countries_hash': 1194, 'rdap_registration_period': Timedelta('11322 days 00:00:00'), 'rdap_domain_age': 9619.833333333334, 'rdap_time_from_last_change': 1590.3478703703704, 'rdap_domain_active_time': 9619.833333333334, 'rdap_has_dnssec': False, 'rdap_registrar_name_len': 16, 'rdap_registrar_name_entropy': 0.21875, 'rdap_registrar_name_hash': 4215097277, 'rdap_registrant_name_len': 0, 'rdap_registrant_name_entropy': 0.0, 'rdap_admin_name_len': 0, 'rdap_admin_name_entropy': 0.0, 'rdap_admin_email_len': 0, 'rdap_admin_email_entropy': 0.0, 'rdap_ip_v4_count': 21, 'rdap_ip_v6_count': 18, 'rdap_ip_shortest_v4_prefix_len': 15.0, 'rdap_ip_longest_v4_prefix_len': 19.0, 'rdap_ip_shortest_v6_prefix_len': 32.0, 'rdap_ip_longest_v6_prefix_len': 37.0, 'rdap_ip_avg_admin_name_len': 7.435897435897436, 'rdap_ip_avg_admin_name_entropy': 0.2172715762864962, 'rdap_ip_avg_admin_email_len': 17.102564102564102, 'rdap_ip_avg_admin_email_entropy': 0.11707256493273657}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 10:53:31,988 - utils.preprocess_one_domain - INFO - Single-record processing: 1 rows\n",
      "2024-05-11 10:53:32,050 - utils.preprocess_one_domain - INFO - Decision tree model loaded from trained_borders/decision_tree_model.joblib\n",
      "2024-05-11 10:53:32,136 - utils.preprocess_one_domain - INFO - Applied loaded decision tree pipeline to generate 'dtree_prob' for the single record.\n",
      "2024-05-11 10:53:32,139 - utils.preprocess_one_domain - INFO - Generated class map: {'benign_2310:unknown': 0}\n",
      "2024-05-11 10:53:32,171 - utils.preprocess_one_domain - INFO - Scaler loaded from trained_borders/scaler.joblib\n",
      "2024-05-11 10:53:32,183 - utils.preprocess_one_domain - INFO - Outliers thresholds loaded from trained_borders/outliers.joblib\n",
      "2024-05-11 10:53:32,292 - utils.preprocess_one_domain - INFO - Completed outlier removal.\n",
      "2024-05-11 10:53:32,323 - utils.preprocess_one_domain - INFO - Applying MinMaxScaler + Sigmoid scaling to the features.\n",
      "2024-05-11 10:53:32,334 - utils.preprocess_one_domain - INFO - Scaler loaded from trained_borders/scaler.joblib\n",
      "2024-05-11 10:53:32,347 - utils.preprocess_one_domain - INFO - Outliers thresholds loaded from trained_borders/outliers.joblib\n",
      "2024-05-11 10:53:32,354 - utils.preprocess_one_domain - INFO - Scaling applied to the features\n",
      "\n",
      "2024-05-11 10:53:32,821 - utils.preprocess_one_domain - INFO - Modified combined dataset saved to modified_dataset.parquet\n",
      "2024-05-11 10:53:32,825 - utils.preprocess_one_domain - INFO - Head of modified combined dataset:\n",
      "2024-05-11 10:53:32,828 - utils.preprocess_one_domain - INFO -    dns_has_dnskey  dns_A_count  dns_AAAA_count  dns_MX_count  dns_NS_count  \\\n",
      "0             0.0     0.071429        0.111111      0.090909      0.307692   \n",
      "\n",
      "   dns_TXT_count  dns_SOA_count  dns_CNAME_count  dns_zone_level  \\\n",
      "0            0.5            1.0              0.0             0.0   \n",
      "\n",
      "   dns_zone_digit_count  ...  rdap_ip_v6_count  \\\n",
      "0                   0.0  ...              0.75   \n",
      "\n",
      "   rdap_ip_shortest_v4_prefix_len  rdap_ip_longest_v4_prefix_len  \\\n",
      "0                         0.46875                        0.59375   \n",
      "\n",
      "   rdap_ip_shortest_v6_prefix_len  rdap_ip_longest_v6_prefix_len  \\\n",
      "0                            0.25                       0.289062   \n",
      "\n",
      "   rdap_ip_avg_admin_name_len  rdap_ip_avg_admin_name_entropy  \\\n",
      "0                    0.107767                        0.411249   \n",
      "\n",
      "   rdap_ip_avg_admin_email_len  rdap_ip_avg_admin_email_entropy  dtree_prob  \n",
      "0                     0.328895                         0.352424         0.0  \n",
      "\n",
      "[1 rows x 179 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Subset:\n",
      "Name: single_record_dataset_2024-05-11\n",
      "Features:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "0        0.0   0.071429   0.111111   0.090909   0.307692        0.5   \n",
      "\n",
      "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_169  Feature_170  \\\n",
      "0        1.0        0.0        0.0        0.0  ...         0.75      0.46875   \n",
      "\n",
      "   Feature_171  Feature_172  Feature_173  Feature_174  Feature_175  \\\n",
      "0      0.59375         0.25     0.289062     0.107767     0.411249   \n",
      "\n",
      "   Feature_176  Feature_177  Feature_178  \n",
      "0     0.328895     0.352424          0.0  \n",
      "\n",
      "[1 rows x 179 columns]\n",
      "Labels:\n",
      "   Label\n",
      "0    0.0\n",
      "Dimension: 179\n",
      "Predicted classes: [0]\n",
      "Probabilities: [[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pyarrow.parquet as pq\n",
    "import joblib\n",
    "\n",
    "# Define or import your Net class and other necessary components here\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, side_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * (side_size-4)**2, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the FeatureEngineeringCLI, NDF function are defined as provided\n",
    "\n",
    "def read_first_record_parquet(parquet_file):\n",
    "    # Read the Parquet file\n",
    "    table = pq.read_table(parquet_file)\n",
    "    first_record = table.to_pandas().iloc[[0]]\n",
    "    return first_record\n",
    "\n",
    "def read_first_record_parquet_dict(parquet_file):\n",
    "    # Read the Parquet file\n",
    "    table = pq.read_table(parquet_file)\n",
    "    first_record_dict = table.to_pandas().iloc[[0]].to_dict(orient='records')[0]\n",
    "    return first_record_dict\n",
    "\n",
    "# Function to calculate the next perfect square greater than a given number\n",
    "def next_perfect_square(n):\n",
    "    next_square = np.ceil(np.sqrt(n))**2\n",
    "    return int(next_square)\n",
    "\n",
    "# Example usage to preprocess and classify a single record\n",
    "parquet_file_path = \"../floor/benign_2312.parquet\"\n",
    "first_record_df = read_first_record_parquet_dict(parquet_file_path)\n",
    "\n",
    "# Preprocess the single record\n",
    "print(\"Preprocessing the single record...\")\n",
    "print(first_record_df)\n",
    "preprocessed_data = NDF(\"cnn\", True, input_data=first_record_df, one_line_processing=True)\n",
    "\n",
    "\n",
    "desired_size = next_perfect_square(feature_size)  # Next perfect square\n",
    "side_size = int(desired_size**0.5)\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = Net(side_size=side_size)\n",
    "model.load_state_dict(torch.load('models/ndf_classification.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the features tensor is in the correct shape for prediction\n",
    "# Reshape logic based on the error encountered\n",
    "data_tensor = preprocessed_data['features']\n",
    "feature_size = data_tensor.shape[1]\n",
    "desired_size = next_perfect_square(feature_size)\n",
    "padding = desired_size - feature_size\n",
    "if padding > 0:\n",
    "    data_tensor_padded = F.pad(data_tensor, (0, padding), 'constant', 0)\n",
    "else:\n",
    "    data_tensor_padded = data_tensor\n",
    "side_size = int(np.sqrt(desired_size))\n",
    "data_tensor_reshaped = data_tensor_padded.view(-1, 1, side_size, side_size)\n",
    "\n",
    "\n",
    "\n",
    "# Predict and calculate probabilities for the single record\n",
    "with torch.no_grad():\n",
    "    outputs = model(data_tensor_reshaped)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    probabilities_np = probabilities.detach().cpu().numpy()\n",
    "    probabilities_np_rounded = np.round(probabilities_np, decimals=3)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_np = predicted.detach().cpu().numpy()\n",
    "\n",
    "print(\"Predicted classes:\", predicted_np)\n",
    "print(\"Probabilities:\", probabilities_np_rounded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
